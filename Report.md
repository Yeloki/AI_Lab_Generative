# Отчёт по лабораторной работе №3

Выполнили:

Студент (ФИО) | Роль в проекте   | Оценка
-------------|---------------------|------
Орусский Вячеслав Русланович | обучал Simple RNN с посимвольной и пословной токенизацией | TBD
Петров Илья Олегович | Обучал однонаправленную однослойную и многослойную LSTM c посимвольной токенизацией и токенизацией по словам и на основе BPE | TBD
Карнаков Никита Дмитриевич | Отчет, обучал двунаправленную LSTM | TBD

В данном отчете будем приводить описание каждой из модели (процесс обучения, фрагменты кода и т.д.)

## LSTM с посимвольной токенизацией

### Предварительная обработка данных 

Из файла [data2.txt](data2.txt) извлекаются строки, игнорируются заголовки, такие как "Глава", и объединяются в один текстовый массив.

```python
raw = open('data2.txt', mode='r', encoding='utf-8').readlines()
data = []
for line in raw:
    if line != '\n' and 'Глава' not in line:
        data.append(' '.join(line.split()[1:]))
    
data = [line.replace('\n', ' ').replace('\xa0', ' ') for line in data]
text = ' '.join(data)
```
Пример:

```python
text[:100]
```

```plaintext
завет ЗАВЕТ Моисея  В начале сотворил Бог небо и землю. Земля же была безвидна и пуста, и тьма над б
```

В коде строится алфавит всех уникальных символов, присутствующих в тексте, и каждому символу присваивается индекс. Токенизация происходит на уровне символов, что позволяет модели работать с текстом по букве.

```python
def get_features_target(seq):
    features = seq[:-1]
    target = seq[1:]
    return features, target

BATCH_SIZE = 128

alphabet = np.array(sorted(set(text)))
sym_to_idx = {}
idx_to_sym = {}

for idx, sym in enumerate(alphabet):
    sym_to_idx[sym] = idx
    idx_to_sym[idx] = sym
    
text_idx = np.array([sym_to_idx[char] for char in text])
```

### Создание обучающих данных

Текст превращается в последовательность индексов символов. Эти последовательности подаются в нейронную сеть с помощью пакетов (батчей).

```python
sequences = Dataset.from_tensor_slices(text_idx).batch(BATCH_SIZE, drop_remainder=True)
dataset = sequences.map(get_features_target)
data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()
data = data.prefetch(AUTOTUNE)
```
### Архитектура модели

Модель состоит из следующих слоёв:

* **Embedding слой**: Преобразует каждый символ в векторное представление;
* **LSTM слой**: Используется для обработки последовательностей и запоминания контекста;
* **Dense слой**: Выдаёт вероятности для каждого символа алфавита на выходе.

```python
model = keras.Sequential([
    keras.layers.Embedding(len(alphabet), 256),
    keras.layers.LSTM(512, return_sequences=True, stateful=True),
    keras.layers.Dense(len(alphabet))
])
```

### Компиляция и обучение модели

Модель компилируется с использованием стандартного оптимизатора `Adam` и функции потерь `SparseCategoricalCrossentropy`, подходящей для многоклассовой классификации.

```python
model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
model.fit(data, epochs=40, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)
```

### Генерация текста

Генерация текста происходит с помощью функции `predict_next`. Модель принимает начальную последовательность символов и предсказывает следующий символ, повторяя процесс до получения полной последовательности.

```python
def predict_next(sample, model, tokenizer, vocabulary, n_next, rnd_power, batch_size):
    sample_token = [tokenizer[char] for char in sample]
    predicted = sample_token

    sample_tensor = tf.expand_dims(sample_token, 0)
    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    
    for _ in range(n_next):
        cur = model(sample_tensor)
        cur = cur[0].numpy() / rnd_power
        cur = tf.random.categorical(cur, num_samples=1)[-1, 0].numpy()
        predicted.append(cur)
        sample_tensor = predicted[-99:]
        sample_tensor = tf.expand_dims([cur], 0)
        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    res = [vocabulary[i] for i in predicted]
    generated = ''.join(res)
    return generated
```

### Примеры предсказаний и анализ

**Пример 1 (с температурой 0.6)**

```python
print(predict_next(
    sample='б',
    model=model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=200,
    temperature=0.6,
    batch_size=BATCH_SIZE
))
```
На выходе получаем следующее:

```plaintext
бом и селениями своими, ибо время согрешающего совершенного скота, о котором сказано: верен Бог их и умер, и освящающих нас, что делается всё, что значит им слово Божие, которое я имею тельцов и не дол
```

Этот пример показывает, что при температуре 0.6 текст получается разнообразным, но временами нарушается логика предложений. Модель способна генерировать осмысленные фразы, однако часть текста может не иметь связного смысла.

**Пример 2 (с температурой 0.2)**

```python
print(predict_next(
    sample='1',
    model=model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=100,
    temperature=0.2,
    batch_size=BATCH_SIZE
))
```
На выходе получаем следующее:

```plaintext
1гая в сердцах своих и принятие в слове своём и сказали ему: вот, я возлюбленному своему и служению в
```

Здесь текст выглядит более предсказуемым, что объясняется низкой температурой (0.2). Однако начальный символ "1" явно выбивается из общего контекста. В то же время фразы более структурированы, хотя разнообразие предсказаний ниже.

### Выводы

Несмотря на работу с символами, модель способна генерировать текст, который напоминает исходные данные (религиозный текст). При этом, чем выше температура, появляются более разнообразные, но менее осмысленные фразы. При генерации длинных последовательностей иногда теряется связность между предложениями, что характерно для моделей, работающих на уровне символов.

## LSTM с пословной токенизацией

### Предварительная обработка данных

Модель работает с текстовыми данными, считанными из файла data2.txt. Текст предварительно очищается, фильтруются лишние символы (оставляются только кириллические символы, цифры и знаки препинания).

```python
raw = open('data2.txt', mode='r', encoding='utf-8').readlines()
data = []
for line in raw:
    if line != '\n' and 'Глава' not in line:
        data.append(' '.join(line.split()[1:]))
    
data = [line.replace('\n', ' ').replace('\xa0', ' ') for line in data]
text = ' '.join(data)
```

Здесь строки текста очищаются от переносов и пробелов, как и в первой модели. Однако есть дополнительный этап очистки текста с помощью регулярного выражения

```python
word_pattern = r'[^а-яА-ЯёЁ0-9 :,-]'
clear_text = re.sub(word_pattern, '', text)
```

Этот код удаляет все символы, которые не соответствуют указанному паттерну, оставляя только кириллические буквы, цифры, пробелы и знаки препинания.

### Токенизация слоев

В этой модели производится токенизация на уровне слов, а не символов, что позволяет модели лучше улавливать смысловые связи между словами.

```python
alphabet = np.array(sorted(set(clear_text.split(' '))))
clear_text = np.array(clear_text.split(' '))
clear_text = clear_text[clear_text != '']

word_to_idx = {}
idx_to_word = {}

for idx, word in enumerate(alphabet):
    word_to_idx[word] = idx
    idx_to_word[idx] = word
```

Здесь модель работает с последовательностями слов, что помогает лучше улавливать контекст по сравнению с символами.

### Создание обучающих данных

Данные преобразуются в последовательности индексов слов, которые подаются в модель через батчи с использованием библиотеки `TensorFlow Dataset`.

```python
text_idx = np.array([word_to_idx[word] for word in clear_text])
sequences = Dataset.from_tensor_slices(text_idx).batch(BATCH_SIZE, drop_remainder=True)
dataset = sequences.map(get_features_target)

data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()
data = data.prefetch(AUTOTUNE)
```

### Архитектура модели

Архитектура модели включает три основных слоя:

* **Embedding слой**: Преобразует слова в векторное представление;
* **LSTM слой**: Используется для обработки последовательностей и запоминания контекста между словами;
* **Dense слой**: Выдаёт вероятности для каждого слова в словаре.

### Комплияция и обучение данных

Модель компилируется с оптимизатором **Adam** и функцией потерь `SparseCategoricalCrossentropy`. Она обучается на всех батчах данных в течение 20 эпох.

```python
model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
model.fit(data, epochs=20, verbose=1, steps_per_epoch=len(sequences) // BATCH_SIZE)
```

`SparseCategoricalCrossentropy` — функция потерь, которая подходит для задачи многоклассовой классификации, где каждый класс соответствует одному слову.

### Генерация текста

Текст генерируется с помощью функции `predict_next`, которая принимает начальную последовательность слов и предсказывает следующее слово на основе вероятностей, выданных моделью. Процесс повторяется, чтобы сгенерировать заданное количество слов.

```python
def predict_next(sample, model, tokenizer, vocabulary, n_next, rnd_power, batch_size):
    sample_token = [tokenizer[char] for char in sample.split()]
    predicted = sample_token

    sample_tensor = tf.expand_dims(sample_token, 0)
    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    
    for _ in range(n_next):
        cur = model(sample_tensor)
        cur = cur[0].numpy() / rnd_power
        cur = tf.random.categorical(cur, num_samples=1)[-1, 0].numpy()
        predicted.append(cur)
        sample_tensor = predicted[-99:]
        sample_tensor = tf.expand_dims([cur], 0)
        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    res = [vocabulary[i] for i in predicted]
    generated = ' '.join(res)
    return generated
```

### Примеры предсказаний и анализ

**Пример 1 с температурой 0.6**

```plaintext
бог его и хула из земли и преславного подчиниться останься в любезно, что длина их в Самом Каину а на земле
```

При генерации текстов с начальным словом "бог" видно, что модель может генерировать осмысленные предложения, но логика иногда сбивается (например, "подчиниться останься в любезно").

**Пример 2 с температурой 0.2**

Начало с цифры "1" также приводит к генерации относительно осмысленного текста, но с частичной утратой связности (например, "когда в потеряв который на него").

### Выводы

Модель генерирует текст на уровне слов. Это улучшает её способность создавать более осмысленные фразы по сравнению с моделью, работающей на уровне символов. Она лучше улавливает контексты между словами, что важно для смысловой целостности текста. Как и в первой модели, при повышении температуры генерации текст становится более разнообразным, но теряет связность. Низкие значения температуры делают текст более предсказуемым, но менее разнообразным. Благодаря токенизации на уровне слов, модель создаёт более логичные и связные предложения, хотя ещё встречаются некоторые ошибки и нарушения логики. Модель, хотя и улавливает общие связи между словами, не всегда правильно строит грамматически верные предложения.

## Двунаправленная LSTM

Третья модель использует посимвольную токенизацию текста и применяет архитектуру нейронной сети с двунаправленной LSTM (Bidirectional LSTM). Эта архитектура позволяет учитывать как прошлые, так и будущие состояния для более точного предсказания следующих символов. Также модель использует встроенный слой `Embedding`, который кодирует символы в векторное пространство, и полносвязный слой `Dense` для предсказания следующего символа.

## Основные компоненты модели

* **Embedding слой**: преобразует символы в плотные векторные представления размером 128;
* **Bidirectional LSTM**: слой LSTM, который обрабатывает последовательность в обоих направлениях — вперед и назад — что помогает лучше учесть контекст;
* **Dense слой**: полносвязный слой с функцией активации ReLU, который преобразует выход LSTM в предсказание вероятностей символов.

Обучение модели происходит следующим образом:

```python
model = keras.Sequential([
    keras.layers.Embedding(len(alphabet), 128),
    keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),
    keras.layers.Dense(len(alphabet), activation='relu')
])

model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
model.fit(data, epochs=10, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)
```

Здесь мы видим, что модель обучается на посимвольных данных, что может оказаться сложным для долгосрочных зависимостей в тексте. Посимвольная токенизация иногда требует больше шагов для предсказания, особенно для языков с длинными словами.

Генерация текста происходит следующим образом:

```python
def predict_next(sample, model, tokenizer, vocabulary, n_next, rnd_power, batch_size):
    sample_token = [tokenizer[char] for char in sample]
    predicted = sample_token

    sample_tensor = tf.expand_dims(sample_token, 0)
    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    
    for _ in range(n_next):
        cur = model(sample_tensor)
        cur = cur[0].numpy() / rnd_power
        cur = tf.random.categorical(cur, num_samples=1)[-1, 0].numpy()
        predicted.append(cur)
        sample_tensor = predicted[-99:]
        sample_tensor = tf.expand_dims([cur], 0)
        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    res = [vocabulary[i] for i in predicted]
    generated = ''.join(res)
    return generated
```

Этот код использует предсказания модели для генерации новых символов, добавляя их к текущей последовательности. Модель предсказывает символы по одному, с учетом предыдущего контекста.

## Примеры предсказаний и анализ

**Пример 1 (на основе символа 'б' с температурой 0.6)**

```python
print(predict_next(
    sample='б',
    model=model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=200,
    temperature=0.6,
    batch_size=BATCH_SIZE
))
```

Получаем следующее:

```plaintext
бо во бов ви по во вовосто Ио мемухововопо оли во и пы е, су дитото сто новя оме ппо, сь во воствогоежетоде ногодеже, вомегоде о намоби потовежего веготв за ва ко воно на води и сенимужегоднаето вестог
```

**Пример 2 (на основе символа '1' с температурой 0.2)**

```python
print(predict_next(
    sample='1',
    model=model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=100,
    temperature=0.2,
    batch_size=BATCH_SIZE
))
```

Получаем следующее:

```plaintext
1щшёP2»Бь0т60*е?э;БшhёiЖ-л@ЧeбохХв_пiАУ?b)4hгоТЛ06руn—Т«М(к,:вР#лiЯоЖэЗРнЭиУ3*oРш0aябИ, uА:йЬ._гпт#р1
```

Результаты генерации в первом примере показали некоторую способность модели к структурированию текста, включая слова, похожие на существующие, например, "воствогоежетоде" и "веготв". Однако на уровне предложений и связности смысл теряется, и модель начинает предсказывать случайные наборы символов.

Когда входной символ — это цифра ("1"), предсказания модели становятся крайне случайными, как видно из вывода: "щшёP2»Бь0т60*е?э;Бш...". Это свидетельствует о проблемах с предсказанием в условиях ограниченного контекста и недостаточной обученности на этом этапе.

ри температуре 0.6 модель производит текст с некоторыми осмысленными словами. Однако при более низкой температуре (0.2) предсказания становятся более детерминированными, но и более случайными, особенно при цифровых входных данных.

## Выводы

В этой модели используется двунаправленная LSTM, что позволяет лучше учитывать контекст как до, так и после текущего символа. В предыдущей модели использовалась однонаправленная LSTM. Теоретически, двунаправленная архитектура может дать более точные предсказания, особенно в сложных языковых задачах, однако результаты показывают, что этого недостаточно для обучения с малым числом эпох и посимвольной токенизацией.

Посимвольная токенизация (как в данной модели) имеет некоторые преимущества, так как она позволяет учитывать мельчайшие детали, но она требует намного больше шагов для генерации текста, что может увеличивать вероятность ошибок. В предыдущей модели использовалась пословная токенизация, которая лучше справляется с длинными зависимостями в языке, хотя и менее точна на уровне отдельных символов.

Как и в предыдущих моделях, генерация текста осуществляется пошагово, добавляя символы к уже сгенерированной последовательности. Однако отсутствие значимой смысловой связности свидетельствует о необходимости дальнейшего улучшения обучения и параметров модели.

Общий вывод можно обощить следующим: эта модель, использующая двунаправленную LSTM и посимвольную токенизацию, показывает потенциал, но пока справляется с задачей предсказания текста хуже, чем предыдущая модель с пословной токенизацией. Основной недостаток этой модели — её неспособность поддерживать долгосрочную связанность текста, особенно при генерации на основе коротких или неинформативных входных данных.

## simple RNN с посимвольной токенизацией

Модель использует рекуррентную нейронную сеть (RNN) с простым RNN слоем для генерации текста на основе посимвольной токенизации. Основные компоненты модели следующие:

* **Embedding слой**: преобразует символы в плотные векторные представления;
* **Simple RNN слой**: используется для обработки последовательностей, чтобы предсказывать следующий символ на основе предыдущих символов. Параметр `return_sequences=True` указывает, что выходной размер будет таким же, как и входной, а `stateful=True` позволяет сохранить состояние RNN между последовательностями;
* **Dense слой**: полносвязный слой, который выводит вероятности для каждого символа в алфавите.

Для обучения модели используется следующий код:

```python
rnn_model = keras.Sequential([
    keras.layers.Embedding(len(alphabet), BATCH_SIZE),
    keras.layers.SimpleRNN(64, return_sequences=True, stateful=True),
    keras.layers.Dense(len(alphabet))
])

rnn_model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
rnn_model.fit(data, epochs=40, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)
```

* **Embedding**: слой эмбеддинга кодирует каждый символ в 128-мерное пространство (здесь это значение равно BATCH_SIZE);
* **SimpleRNN**: слой простой RNN с 64 единицами, который позволяет моделировать последовательные данные. Параметр stateful=True помогает поддерживать состояние между батчами, что может быть полезно при работе с последовательностями;
* **Dense**: последний слой, который используется для предсказания вероятностей следующего символа.

Функция для предсказания следующего символа на основе входного образца:

```python
def predict_next(sample, model, tokenizer, vocabulary, n_next, rnd_power, batch_size):
    sample_token = [tokenizer[char] for char in sample]
    predicted = sample_token

    sample_tensor = tf.expand_dims(sample_token, 0)
    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    
    for _ in range(n_next):
        cur = model(sample_tensor)
        cur = cur[0].numpy() / rnd_power
        cur = tf.random.categorical(cur, num_samples=1)[-1, 0].numpy()
        predicted.append(cur)
        sample_tensor = predicted[-99:]
        sample_tensor = tf.expand_dims([cur], 0)
        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    res = [vocabulary[i] for i in predicted]
    generated = ''.join(res)
    return generated
```
* `sample`: начальный символ для генерации текста.
* `model`: обученная модель RNN;
* `tokenizer`: словарь, который сопоставляет символы с индексами;
* `vocabulary`: обратный словарь для преобразования индексов обратно в символы;
* `n_next`: количество символов для генерации;
* `rnd_power`: параметр, влияющий на случайность предсказания (температура);
* `batch_size`: размер батча.

## Примеры 

**Пример 1 (генерация на основе символа "б") с температурой 0.6**

```python
print(predict_next(
    sample='б',
    model=rnn_model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=200,
    rnd_power=0.6,
    batch_size=BATCH_SIZE
))
```

**Пример 1 (генерация на основе символа "1" с температурой 0.2)**

```python
print(predict_next(
    sample='1',
    model=rnn_model,
    tokenizer=sym_to_idx,
    vocabulary=idx_to_sym,
    n_next=100,
    rnd_power=0.2,
    batch_size=BATCH_SIZE
))
```

Важно отметить, что RNN, как правило, может иметь проблемы с долгосрочной зависимостью. Это может приводить к тому, что модель начинает "забывать" более ранние символы при генерации длинных последовательностей.

## Выводы

Простая RNN модель, хотя и может быть полезна для небольших последовательностей, может не показывать хороших результатов при работе с длинными текстами из-за проблемы с долгосрочной памятью. Использование LSTM или GRU может быть более подходящим для таких задач.

Обучение на 40 эпохах может быть достаточным, но требует мониторинга для избежания переобучения. Необходимо следить за кривыми обучения, чтобы убедиться, что модель не теряет способность обобщать.

Хотя посимвольная токенизация позволяет генерировать текст на более детальном уровне, она требует больше шагов для достижения связного текста. Это может быть проблемой для RNN архитектур.

Общий вывод можно сформулировать следующим образом: модель, использующая простой RNN, имеет свои преимущества, но также и ограничения, особенно в контексте генерации текстов на основе длительных последовательностей. Для улучшения качества предсказаний и генерации текста стоит рассмотреть возможность перехода на более сложные архитектуры, такие как LSTM или GRU, а также использовать методы предобучения и аугментации данных для повышения общей эффективности модели.

## simple RNN  с пословной токенизацией

### Предобработка данных

В данной модели ключевым отличием является пословная токенизация. Вместо обработки текста на уровне символов, как это было в предыдущей модели, здесь токенизация выполняется на уровне слов:

```python
word_pattern = r'[^а-яА-ЯёЁ0-9 :,-]'
clear_text = re.sub(word_pattern, '', text)
```

Регулярное выражение удаляет все символы, кроме русских букв, цифр и некоторых знаков пунктуации.

 Вместо посимвольной разбивки текста, используется токенизация по пробелам:

```python
clear_text = np.array(clear_text.split(' '))
clear_text = clear_text[clear_text != '']
```

Это позволяет обрабатывать каждое слово как отдельный элемент, что способствует лучшей передаче контекста в тексте.

Каждое слово из текста связывается с индексом:

```python
word_to_idx = {}
idx_to_word = {}

for idx, word in enumerate(alphabet):
    word_to_idx[word] = idx
    idx_to_word[idx] = word
```

Это необходимо для последующего преобразования текста в числовые последовательности, которые можно подавать на вход модели.

### Архитектура модели

**Embedding слой**

```python
keras.layers.Embedding(len(alphabet), BATCH_SIZE)
```

Этот слой преобразует слова в векторы, которые затем подаются в рекуррентную сеть. Размер векторов определяется параметром `BATCH_SIZE`.

**SimpleRNN слой**

```python
keras.layers.SimpleRNN(128, return_sequences=True, stateful=True)
```

Основной слой, который обрабатывает последовательности и сохраняет состояние между шагами. Он также возвращает всю последовательность, чтобы иметь возможность предсказать каждое слово в тексте.

**Dense слой с активацией `relu`**

```python
keras.layers.Dense(len(alphabet), activation='relu')
```

Полносвязный слой, который предсказывает вероятность каждого слова в словаре, используя функцию активации `relu`.

### Компиляция и обучение модели

Модель компилируется с использованием оптимизатора `Adam` и функции потерь `SparseCategoricalCrossentropy`, которая подходит для задач классификации:

```python
model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
```

Затем модель обучается на предварительно обработанных данных:

```python
model.fit(data, epochs=20, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)
```

Функция предсказания генерирует текст по одному слову за раз, используя ранее обученную модель:

```python
def predict_next(sample, model, tokenizer, vocabulary, n_next, rnd_power, batch_size):
    sample_token = [tokenizer[char] for char in sample.split()]
    predicted = sample_token

    sample_tensor = tf.expand_dims(sample_token, 0)
    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)

    for _ in range(n_next):
        cur = model(sample_tensor)
        cur = cur[0].numpy() / rnd_power
        cur = tf.random.categorical(cur, num_samples=1)[-1, 0].numpy()
        predicted.append(cur)
        sample_tensor = predicted[-99:]
        sample_tensor = tf.expand_dims([cur], 0)
        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)
    res = [vocabulary[i] for i in predicted]
    generated = ' '.join(res)
    return generated
```

Здесь используется метод сэмплирования, который регулируется параметром `rnd_power` для контроля уровня случайности предсказаний.

### Примеры использования

**Пример 1**

```python
print(predict_next(
    sample='бог',
    model=model,
    tokenizer=word_to_idx,
    vocabulary=idx_to_word,
    n_next=20,
    temperature=0.6,
    batch_size=BATCH_SIZE,
    word=True
))
```

Получаем на выходе следующее:

```plaintext
бог и ложной тот не должно быть от Бога и не только как бы ты и есть Бог о вас, как
```

Модель генерирует текст, содержащий осмысленные слова, но последовательность предложений не всегда логически связана.

**Пример 2**

```python
print(predict_next(
    sample='1',
    model=model,
    tokenizer=word_to_idx,
    vocabulary=idx_to_word,
    n_next=20,
    temperature=0.6,
    batch_size=BATCH_SIZE,
    word=True
))
```

На выходе получаем следующее:

```plaintext
1 себя Деве и всякое вы не будет более и не только более вас, чтобы никто не будет во плоти ибо
```

В этом примере, несмотря на присутствие отдельных слов, логическая целостность текста также сохраняется частично, но последовательность имеет смысловые пробелы.

### Выводы

В данной модели использована пословная токенизация, что позволяет обучать модель на уровне слов, а не отдельных символов. Это улучшает способность модели формировать осмысленные фразы, поскольку она имеет дело с уже структурированными элементами языка (словами), а не с отдельными буквами. Модель может лучше запоминать и предсказывать структуру предложений, так как слова обладают более высоким уровнем семантического содержания по сравнению с символами.

В предыдущей модели использовалась посимвольная токенизация, где каждый символ обрабатывался как отдельный элемент. Это позволяло модели предсказывать точные последовательности символов, но осмысленность генерируемого текста страдала, так как модель должна была «строить» слова из символов, что добавляло сложности. Пословная токенизация улучшает осмысленность выходных данных. Модель работает с готовыми словами, что позволяет ей генерировать более логичные предложения. Однако, как и в модели с посимвольной токенизацией, всё ещё остаются проблемы с долгосрочной памятью — модель забывает контекст при работе с длинными последовательностями, что приводит к логическим ошибкам в тексте.

Модель использует слой `SimpleRNN`, который имеет ограниченную способность запоминать долгосрочные зависимости. Из-за этого обе модели (с посимвольной и пословной токенизацией) сталкиваются с проблемами при работе с длинными текстами. Для улучшения этой проблемы можно было бы использовать более сложные архитектуры, такие как LSTM или GRU, которые лучше справляются с сохранением контекста на больших промежутках времени.

Общий вывод можно смоделировать следующим образом: использование пословной токенизации в данной модели значительно улучшает осмысленность выходного текста по сравнению с моделью, использующей посимвольную токенизацию. Однако проблемы с потерей контекста на длинных последовательностях остаются из-за ограничений архитектуры SimpleRNN. Для дальнейших улучшений можно рассмотреть переход на более сложные рекуррентные сети (LSTM или GRU), а также настройку других гиперпараметров модели, таких как размер векторов слов и число нейронов в рекуррентных слоях.
